{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de disponibilização, catalogação e preprocessamento de relatorios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de captura e disponibilização de relatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root = os.path.dirname(\"C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\Mestrado - NLP - Finance\\\\datasets\\\\\")\n",
    "#levelone = ['10456', '24600', '906', '1210', '1325', '21199', '18724', '4820', '2437', '2453', '3069', '3077', '18376', '16632', '5770', '6211', '3980', '8672', '19348', '7617', '8656', '11312', '14109', '14320']\n",
    "levelone = ['19348']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para download da relação de todos os links\n",
    "def busca_ipe(ano):\n",
    "    url = ('https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/IPE/DADOS/ipe_cia_aberta_%d.zip' %(ano))\n",
    "    file = ('ipe_cia_aberta_%d.zip' %(ano))    \n",
    "    r = requests.get(url)\n",
    "    zf = zipfile.ZipFile(io.BytesIO(r.content))   \n",
    "    file = zf.namelist()  \n",
    "    zf = zf.open(file[0])\n",
    "    lines = zf.readlines()\n",
    "    lines=[i.strip().decode('ISO-8859-1') for i in lines] \n",
    "    lines=[i.split(';') for i in lines]\n",
    "    qtl = len(lines)     \n",
    "    return lines\n",
    "\n",
    "# Função para busca de um item específico\n",
    "def search (lista, valor):\n",
    "    return [(lista[lista.index(x)]) for x in lista if valor in x]\n",
    "\n",
    "# Função para baixar o arquivivo específico\n",
    "def baixar_arquivo(url, endereco):\n",
    "    resposta = requests.get(url, allow_redirects=True, verify=False, stream=True)\n",
    "    if resposta.status_code == requests.codes.OK:\n",
    "        with open(endereco, 'wb') as novo_arquivo:\n",
    "                novo_arquivo.write(resposta.content)\n",
    "        print(\"Download finalizado. Arquivo salvo em: {}\".format(endereco))\n",
    "    else:\n",
    "        resposta.raise_for_status()\n",
    "        \n",
    "#Funcão para baixar arquivos específicos\n",
    "def download_def(year):\n",
    "    lines = busca_ipe(year)\n",
    "    defi = search(lines,\"Dados Econômico-Financeiros\")\n",
    "    year = year\n",
    "    for a in range(len(defi)):\n",
    "       if defi[a][2] in levelone:\n",
    "            sufix = re.sub(u'[^a-zA-Z0-9]','_',defi[a][2]+\"_\"+defi[a][1])\n",
    "            category = re.sub(u'[^a-zA-Z0-9çãàáêéíõóú]','_',defi[a][5])\n",
    "            print(sufix)\n",
    "            company = os.path.join(root + \"\\\\%s\"%(sufix))\n",
    "            print(company)\n",
    "       if not Path(root + \"\\\\%s\"%(sufix)).is_dir():\n",
    "            print(\"nao existe\")\n",
    "            os.makedirs(root + \"\\\\%s\"%(sufix))\n",
    "       if not Path(company +\"\\\\%d\"%(year)).is_dir():\n",
    "            print(\"nao existe\")\n",
    "            os.makedirs(company + \"\\\\%d\"%(year))\n",
    "       diryear = os.path.join(company + \"\\\\%d\"%(year))\n",
    "       if not Path(diryear +\"\\\\%s\"%(category)).is_dir():\n",
    "            print(\"nao existe\")\n",
    "            os.makedirs(diryear + \"\\\\%s\"%(category))\n",
    "       dircategory = os.path.join(diryear + \"\\\\%s\"%(category))\n",
    "       url = defi[a][12]       \n",
    "       nome = re.sub(u'[^a-zA-Z0-9]', '_', defi[a][2]+\"_\"+defi[a][1]+\"_\"+defi[a][7][1:50]+\"_\"+defi[a][8])[1:100]       \n",
    "       baixar_arquivo(url, dircategory+\"\\\\%s.pdf\"%(nome))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de preprocessamento de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas e modelos\n",
    "\n",
    "\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "# Carregar o tokenizer e o modelo pré-treinado\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForPreTraining, BertForQuestionAnswering, AutoModel, AutoTokenizer, BertTokenizer, BertForQuestionAnswering, BertForSequenceClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import PyPDF2\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "# Carrrefa os modelos\n",
    "#model = AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "#model = BertForSequenceClassification.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "model_forquestion = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "#model = AutoModelForPreTraining.from_pretrained(\"neuralmind/bert-large-portuguese-cased\")\n",
    "\n",
    "# Instanciar o modelo e o tokenizador T5 pré-treinado\n",
    "modelsum = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizersum = T5Tokenizer.from_pretrained('t5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_pdf (doc):\n",
    "    with open(doc, \"rb\") as f:\n",
    "        # Use a biblioteca PyPDF2 para ler o arquivo\n",
    "        #reader = PyPDF2.PdfFileReader(f)\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        # Obtenha o número de páginas no arquivo\n",
    "        #num_pages = reader.getNumPages()\n",
    "        num_pages = len(reader.pages)\n",
    "        # Crie uma string vazia para armazenar o texto extraído\n",
    "        text = ''\n",
    "        # Itere pelas páginas e extraia o texto\n",
    "        for page in range(num_pages):\n",
    "            # Obtenha o objeto da página\n",
    "            #page_obj = reader.getPage(page)\n",
    "            page_obj = reader.pages[page]\n",
    "            # Extraia o texto da página\n",
    "            #page_text = page_obj.extractText() \n",
    "            page_text = page_obj.extract_text()\n",
    "            # Adicione o texto extraído à string de texto\n",
    "            text += page_text\n",
    "    return text\n",
    "            \n",
    "\n",
    "def remover_stopwords(tokens):\n",
    "    # Carrega o modelo do SpaCy para o idioma português\n",
    "    nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "    # Cria uma lista para armazenar os tokens sem stopwords\n",
    "    tokens_sem_stopwords = []\n",
    "\n",
    "    # Percorre cada token na lista\n",
    "    for token in tokens:\n",
    "        # Verifica se o token não é uma stopword\n",
    "        if not nlp.vocab[token].is_stop:\n",
    "            tokens_sem_stopwords.append(token)\n",
    "    return tokens_sem_stopwords\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text=text\n",
    "    # Converte o texto para minúsculas\n",
    "    normalized_text = text.lower()\n",
    "    # Remove caracteres especiais e acentuação\n",
    "    normalized_text = re.sub(r'[^\\w\\s]', '', normalized_text)\n",
    "    # Retorna o texto limpo\n",
    "    return normalized_text\n",
    "\n",
    "def tokengen(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True) \n",
    "    return tokens\n",
    "\n",
    "def vector___one(text):\n",
    "    max_length = 512\n",
    "    input_parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    # Lista para armazenar os vetores médios\n",
    "    vectors = []\n",
    "    # Processa cada parte do texto\n",
    "    for part in input_parts:\n",
    "    # Tokeniza a parte do texto\n",
    "        tokens = tokenizer.encode(part, add_special_tokens=True)\n",
    "        input_ids = torch.tensor([tokens])\n",
    "        # Gera os vetores de embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            last_hidden_states = outputs[0]\n",
    "        # Calcula o vetor médio\n",
    "        average_vector = torch.mean(last_hidden_states, dim=1)\n",
    "        # Adiciona o vetor médio à lista\n",
    "        vectors.append(average_vector)\n",
    "    # Concatena os vetores gerados\n",
    "    text_vector = torch.cat(vectors, dim=1)\n",
    "    return text_vector\n",
    "\n",
    "\n",
    "def vector_one(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    max_length = 4096\n",
    "   \n",
    "    # Divide os tokens em partes de tamanho 4096\n",
    "    token_parts = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    \n",
    "    # Converte as partes de tokens em tensores\n",
    "    tensor_parts = [torch.tensor(part) for part in token_parts]\n",
    "    \n",
    "    # Concatena os tensores das partes em um único tensor\n",
    "    concatenated_tensor = torch.cat(tensor_parts)\n",
    "    concatenated_tensor.shape\n",
    "    print(len(tokens))\n",
    "    return concatenated_tensor\n",
    "    \n",
    "\n",
    "def vector____one(text):\n",
    "    max_length = 4096\n",
    "    average_vector = []\n",
    "    vectors = []\n",
    "    input_parts = []\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=4096, padding='longest', return_tensors='pt')\n",
    "    input_ids = tokens.clone().detach()\n",
    "    \n",
    "    # Divide the tokenized text into parts\n",
    "    input_parts = [tokens[0, i:i+max_length] for i in range(0, tokens.shape[1], max_length)]\n",
    "    \n",
    "    # Process each part of the tokenized text\n",
    "    for part in input_parts:\n",
    "        # Generate the embeddings vectors\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids[:, part])\n",
    "            last_hidden_states = outputs[0]\n",
    "        \n",
    "        # Calculate the average vector\n",
    "        average_vector = torch.mean(last_hidden_states, dim=1)\n",
    "        \n",
    "        # Add the average vector to the list\n",
    "        vectors.append(average_vector)\n",
    "    \n",
    "    # Concatenate the generated vectors\n",
    "    text_vector = torch.cat(vectors, dim=1)\n",
    "    return text_vector \n",
    "\n",
    "\n",
    "def similarity_vector(a, b):\n",
    "    size_a = a.size(0)\n",
    "    size_b = b.size(0)\n",
    "    if size_a == size_b:\n",
    "        #similarity_score = cosine_similarity(a, b)\n",
    "        similarity_score = cosine_similarity(a.float(), b.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a > size_b:\n",
    "        diff = size_a - size_b\n",
    "        padded = F.pad(b, pad=(0, diff), mode='constant', value=0)\n",
    "        similarity_score = cosine_similarity(a.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    if size_a < size_b:\n",
    "        diff = size_b - size_a\n",
    "        padded = F.pad(a, pad=(0, diff), mode='constant', value=0)\n",
    "        similarity_score = cosine_similarity(b.float(), padded.float())\n",
    "        return similarity_score.item()\n",
    "    \n",
    "\n",
    "    \n",
    "def question(text, question):\n",
    "    question = question\n",
    "    max_answer_length = 512\n",
    "    max_length = 512\n",
    "    input_parts = []\n",
    "    input_parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "    # Lista para armazenar as respostas\n",
    "    answers = []\n",
    "    # Processa cada parte do texto\n",
    "    for part in input_parts:\n",
    "        # Tokeniza a parte do texto\n",
    "        # Tokeniza a pergunta e o texto menor\n",
    "        encoding = tokenizer.encode_plus(question, part, return_tensors='pt', max_length=512, truncation=True)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        # Obtém as respostas do modelo pré-treinado\n",
    "        with torch.no_grad():\n",
    "            outputs = model_forquestion(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "        # Obtém a resposta com maior probabilidade\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start.item():answer_end.item()+1]))\n",
    "        # Adiciona a resposta à lista\n",
    "        answers.append(answer)\n",
    "    # Combina as respostas\n",
    "    combined_answer = ' '.join(answers)\n",
    "    # Limita o tamanho da resposta final\n",
    "    combined_answer = combined_answer[:max_answer_length]\n",
    "        # Formatação da resposta final\n",
    "    formatted_answer = \"RESPOSTA:\\n\\n\"\n",
    "    formatted_answer += combined_answer.replace(\".\", \".\\n\\n\")\n",
    "    # Apresenta a resposta final\n",
    "    return formatted_answer\n",
    "    # Apresenta a resposta final\n",
    "    return combined_answer\n",
    "\n",
    "\n",
    "def summarization(text):\n",
    "    num_sentences=5\n",
    "    #max_length=512\n",
    "    # Tokenizar o texto em partes\n",
    "    input_parts = text.split('. ')\n",
    "    summaries = []\n",
    "    #input_parts = []\n",
    "    #input_parts = [text[i:i+max_length] for i in range(0, len(text), max_length)]    \n",
    "    for part in input_parts:\n",
    "        lenght=len(part)\n",
    "        m_length = int(lenght*0.4)\n",
    "        # Tokenizar a parte do texto\n",
    "        inputs = tokenizersum.encode(\"summarize: \" + part, return_tensors=\"pt\")\n",
    "        # Gerar o resumo da parte do texto usando o modelo T5\n",
    "        outputs = modelsum.generate(inputs, max_length=m_length, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizersum.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Adicionar o resumo à lista de sumários\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Concatenar os sumários em um único texto\n",
    "    concatenated_summary = ' '.join(summaries)\n",
    "\n",
    "    # Extrair as N sentenças mais importantes\n",
    "    sentences = concatenated_summary.split('. ')\n",
    "    top_sentences = sorted(sentences, key=lambda x: len(x), reverse=True)[:num_sentences]\n",
    "\n",
    "    return '. '.join(top_sentences)\n",
    "\n",
    "\n",
    "def prep_proc(text1,text2):\n",
    "    nm1=normalize_text(text1)\n",
    "    nm2=normalize_text(text2)\n",
    "    print(\"Texto normalizado 1: \" + nm1[:200])\n",
    "    print(\"Texto normalizado 1: \" + nm2[:200])\n",
    "    tk1 = tokengen(nm1)\n",
    "    tk2 = tokengen(nm2)\n",
    "    vt1=vector_one(nm1)\n",
    "    vt2=vector_one(nm2)\n",
    "    print(\"Vetor do artigo 1:\")\n",
    "    print(vt1.shape)\n",
    "    print(v1)\n",
    "    print(\"Vetor do artigo 2:\")\n",
    "    print(vt2.shape)\n",
    "    print(v2)\n",
    "    print(\"calculo de similaridades entre texto 1 e texto2 (cosseno)\")\n",
    "    sim=similarity_vector(v1,v2)\n",
    "    print(sim)\n",
    "    return v1,v2,sim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serviço de catalogação, funções de indexação e busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\solr -c -z localhost:9983 -p 8984', returncode=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Inicialização do serviço de catalogação modo cloud\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Comando que você deseja executar\n",
    "command = 'C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\solr restart -port 8983'\n",
    "#Inicialização em modo cloud\n",
    "command2 = 'C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\solr -c -z localhost:9983 -p 8984'\n",
    "\n",
    "# Executa o comando no terminal\n",
    "subprocess.run(command, shell=True)\n",
    "#Inicialização em modo cloud\n",
    "subprocess.run(command2, shell=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação da coleção (agrupamento):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para o diretório bin do Solr\n",
    "solr_bin_path = 'C:\\\\Users\\\\thgcn\\\\OneDrive\\\\Academico\\\\PO-245\\\\Projeto\\\\solr-9.2.1\\\\bin\\\\'\n",
    "\n",
    "# Comando para criar a coleção no modo standalone\n",
    "command = 'solr create_collection -c relatoriosfinanceiros -s 2 -rf 2'\n",
    "\n",
    "os.chdir(solr_bin_path)\n",
    "os.system(command)\n",
    "\n",
    "# URL do endpoint Solr\n",
    "url = 'http://localhost:8983/api/cores/relatoriosfinanceiros/schema'\n",
    "\n",
    "# Cabeçalhos da solicitação POST\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Corpo dos campos da catalogação\n",
    "data = {\n",
    "    \"add-field\": [\n",
    "        {\"name\": \"report_id\", \"type\": \"string\"},\n",
    "        {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "        {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "        {\"name\": \"cod_cvm\", \"type\": \"text_general\", \"multiValued\": False},\n",
    "        {\"name\": \"year\", \"type\": \"pint\"},\n",
    "        {\"name\": \"date\", \"type\": \"pdate\"},\n",
    "        {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "        {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "        {\"name\": \"size\", \"type\": \"text_general\", \"multiValued\": True},\n",
    "        {\"name\": \"tensor\", \"type\": \"text_general\",\"multiValued\": True}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Enviar solicitação POST para atualizar o esquema no Solr\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "if response.status_code == 200:\n",
    "    print(\"Esquema atualizado com sucesso.\")\n",
    "else:\n",
    "    print(\"Falha ao atualizar o esquema. Status:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falha ao atualizar o esquema. Status: 400\n"
     ]
    }
   ],
   "source": [
    "def update_schema(report_id, title, company_name, year, date, content, keywords, size, tensor):\n",
    "    # URL do endpoint Solr\n",
    "    url = 'http://localhost:8983/api/cores/relatoriosfinanceiros/schema'\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Corpo da solicitação POST\n",
    "    data = {\n",
    "        \"add-field\": [\n",
    "            {\"name\": \"report_id\", \"type\": \"string\", \"value\": report_id},\n",
    "            {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False, \"value\": title},\n",
    "            {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False, \"value\": company_name},\n",
    "            {\"name\": \"year\", \"type\": \"pint\", \"value\": year},\n",
    "            {\"name\": \"date\", \"type\": \"pdate\", \"value\": date},\n",
    "            {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True, \"value\": content},\n",
    "            {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True, \"value\": keywords},\n",
    "            {\"name\": \"size\", \"type\": \"text_general\", \"multiValued\": True, \"value\": size},\n",
    "            {\"name\": \"tensor\", \"type\": \"text_general\", \"multiValued\": True, \"value\": tensor}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Enviar solicitação POST para atualizar o esquema no Solr\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Esquema atualizado com sucesso.\")\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema. Status:\", response.status_code)\n",
    "\n",
    "# Exemplo de uso da função\n",
    "update_schema(\"123456\", \"Relatório de Vendas\", \"Itau\", 2023, \"2023-05-15\", \"Relatorio anual de 2022\", \"relatorio financeiro itau finanças\", \"104857600\", \"12751832\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def update_schema(report_id, title, company_name, year, date, content, keywords, size, tensor):\n",
    "    # URL do endpoint Solr\n",
    "    url = 'http://localhost:8983/api/cores/relatoriosfinanceiros/schema'\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Corpo da solicitação POST\n",
    "    data = {\n",
    "        \"add-field\": [\n",
    "            {\"name\": \"report_id\", \"type\": \"string\", \"value\": report_id},\n",
    "            {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False, \"value\": title},\n",
    "            {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False, \"value\": company_name},\n",
    "            {\"name\": \"year\", \"type\": \"pint\", \"value\": year},\n",
    "            {\"name\": \"date\", \"type\": \"pdate\", \"value\": date},\n",
    "            {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True, \"value\": content},\n",
    "            {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True, \"value\": keywords},\n",
    "            {\"name\": \"size\", \"type\": \"text_general\", \"multiValued\": True, \"value\": size},\n",
    "            {\"name\": \"tensor\", \"type\": \"text_general\", \"multiValued\": True, \"value\": tensor}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Serializar o corpo da solicitação POST como uma string JSON\n",
    "    data_json = json.dumps(data)\n",
    "\n",
    "    # Comando curl para enviar a solicitação POST\n",
    "    command = f\"curl --request POST --url '{url}' --header 'Content-Type: application/json' --data '{data_json}'\"\n",
    "\n",
    "    # Executar o comando usando os.system()\n",
    "    os.system(command)\n",
    "\n",
    "# Exemplo de uso da função\n",
    "update_schema(\"123456\", \"Relatório de Vendas\", \"Itau\", 2023, \"2023-05-15\", \"Relatorio anual de 2022\", \"relatorio financeiro itau finanças\", \"104857600\", \"12751832\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def update_schema(report_id, title, company_name, year, date, content, keywords, size, tensor):\n",
    "    # URL do endpoint Solr\n",
    "    url = 'http://localhost:8983/api/cores/relatoriosfinanceiros/schema'\n",
    "\n",
    "    # Cabeçalhos da solicitação POST\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Corpo da solicitação POST\n",
    "    data = {\n",
    "        \"add-field\": [\n",
    "            {\"name\": \"report_id\", \"type\": \"string\", \"value\": report_id},\n",
    "            {\"name\": \"title\", \"type\": \"text_general\", \"multiValued\": False, \"value\": title},\n",
    "            {\"name\": \"company_name\", \"type\": \"text_general\", \"multiValued\": False, \"value\": company_name},\n",
    "            {\"name\": \"year\", \"type\": \"pint\", \"value\": year},\n",
    "            {\"name\": \"date\", \"type\": \"pdate\", \"value\": date},\n",
    "            {\"name\": \"content\", \"type\": \"text_general\", \"multiValued\": True, \"value\": content},\n",
    "            {\"name\": \"keywords\", \"type\": \"text_general\", \"multiValued\": True, \"value\": keywords},\n",
    "            {\"name\": \"size\", \"type\": \"text_general\", \"multiValued\": True, \"value\": size},\n",
    "            {\"name\": \"tensor\", \"type\": \"text_general\", \"multiValued\": True, \"value\": tensor}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Serializar o corpo da solicitação POST como uma string JSON\n",
    "    data_json = json.dumps(data)\n",
    "\n",
    "    # Comando curl para enviar a solicitação POST\n",
    "    command = f\"curl --request POST --url '{url}' --header 'Content-Type: application/json' --data '{data_json}'\"\n",
    "\n",
    "    # Exibir o comando curl para depuração\n",
    "    print(\"Comando curl:\", command)\n",
    "\n",
    "    # Executar o comando usando os.system()\n",
    "    return_code = os.system(command)\n",
    "\n",
    "    # Verificar o código de retorno\n",
    "    if return_code == 0:\n",
    "        print(\"Esquema atualizado com sucesso.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Falha ao atualizar o esquema.\")\n",
    "        return False\n",
    "\n",
    "# Exemplo de uso da função\n",
    "update_successful = update_schema(\"123456\", \"Relatório de Vendas\", \"Itau\", 2023, \"2023-05-15\", \"Relatorio anual de 2022\", \"relatorio financeiro itau finanças\", \"104857600\", \"12751832\")\n",
    "\n",
    "if update_successful:\n",
    "    # A solicitação foi bem-sucedida\n",
    "    print(\"Solicitação executada com sucesso.\")\n",
    "else:\n",
    "    # A solicitação falhou\n",
    "    print(\"Falha na execução da solicitação.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(empresa, ano):\n",
    "    doc = download_def(empresa, ano)\n",
    "    nm=normalize_text(doc)\n",
    "    print(\"Texto normalizado: \" + nm[:200])\n",
    "    print(\"Tokens texto:\")\n",
    "    tk=tokengen(nm)\n",
    "    vt=vector_one(nm)\n",
    "    print(\"Vetor do artigo:\")\n",
    "    print(vt.shape)\n",
    "    print(vt)\n",
    "    report_id = doc[0]\n",
    "    title = doc[1]\n",
    "    date = doc[2]\n",
    "    type =doc[3]\n",
    "    tags = doc[4]\n",
    "    size =doc[5]    \n",
    "    update_schema(report_id, title, empresa, ano, date, type, tags, size, vt)\n",
    "\n",
    "    return doc, report_id, title, date, type, tags, size, vt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chamada da função\n",
    "\n",
    "pipeline(itau, 2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
